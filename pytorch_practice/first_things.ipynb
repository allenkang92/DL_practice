{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 프레임워크란?\n",
    "딥러닝 뿐만 아니라 다양한 웹, 앱, 네트워크 등 다양한 프로그래밍 분야가 존재함.\n",
    "프로그램을 다룸에 있어서, 공통적으로 사용되는 기능들을 표준화된 소스코드로 만들어 놓고 사용할 수 있도록 제공하는 것 \n",
    "== 프레임워크\n",
    "\n",
    "딥러닝에도 기초 함수부터 복잡한 신경망 등이 구현되어 있는 여러 가지 프레임워크가 있고 pytorch도 그 중 하나임.\n",
    "\n",
    "2) 딥러닝 프레임워크의 종류와 특징\n",
    "개발에 사용되는 언어와 사용하는 방식에 따라 설계가 조금씩 달라지기 때문에 딥러닝 프레임워크도 그 숫자가 많다.\n",
    "그 중 대표적으로 사용되는 프레임워크는 텐서플로우, 파이토치 두 가지가 있음. \n",
    "\n",
    "![poster](./import_first.png)\n",
    "\n",
    "3) Define and Run vs. Define by Run\n",
    "Define and Run의 특징을 가지는 텐서플로우의 경우,\n",
    "- 실행할 계산에 관련된 그래프를 미리 정의하여 올려놓고\n",
    "- 그래프에 투입될 데이터들을 집어넣어 연산을 수행하는 방식.\n",
    "- 따라서, 한 번 실행이 된 상태에서 에러가 나면 찾기가 힘들다!\n",
    "\n",
    "반면 Define by Run의 특성을 가지고 있는 파이토치의 경우,\n",
    "- 연산이 이루어지는 시점에서 동적으로 그래프를 만들어 연산을 수행하기 때문에\n",
    "- 조금 더 낮은 단위의 연산들로 구성할 수 있게 되고\n",
    "- 디버깅 및 구조 설계의 세분화가 가능해짐!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Tensor\n",
    "파이토치를 공부하면서 가장 많이 볼 객체 중 하나. Tensor는 numpy에서 ndarray, 파이썬 내장 객체의 list와 유사한 배열 객체. \n",
    "엄밀히 말하면, list 객체보다 numpy의 ndarray와 가장 유사하다고 볼 수 있음. 따라서 Tensor 객체가 가지고 있는 여러 메서드나 Tensor 객체를 통해서 이루어지는 연산들은 numpy의 ndarray와 거의 동일.\n",
    "\n",
    "\n",
    "2) Squeeze vs Unsqueeze\n",
    "\n",
    "딥러닝을 학습하면서, 다양한 행렬 연산들을 수행하게 됨. \n",
    "그런데 행렬 연산시, 가장 중요한 게 행렬의 크기입니다. 행렬의 크기가 다른 경우, 다른 연산의 결과가 수행될 수 도 있습니다. 이를 위해서 PyTorch에서는 다양한 방식의 함수를 제공. \n",
    "그 중의 하나로 Squeeze와 Unsqueeze, 이를 통해서 행렬의 \"차원\"을 하나 더 높이거나 줄 일 수 있습니다. 예를들어 [1,2,3,4] 와 같은 행렬을 [[1,2,3,4]] 또는 [[1],[2],[3],[4]] 와 같이 변환 할 수 있습니다. 물론 그 반대도 가능.\n",
    "\n",
    "3) Broadcasting\n",
    "\n",
    "torch를 통해 행렬을 연산하다 보면, 두 행렬의 크기가 갖지 않아도 계산이 되는 경우가 종종 있습니다. 이 경우 작은 크기의 행렬이 큰 크기의 행렬의 차원으로 Broadcasting 되는 것인데요, 다음 예시를 통해 살펴보겠습니다. 만약 A라는 tensor에 2X4 크기의 다음과 같은 행렬 [[1,2,3,4],[5,6,7,8]] 이 할당되어 있고, B라는 행렬에 1X4 크기의 다음과 같은 [1,2,3,4] 라는 행렬이 할당 되어 있다고 해봅시다. 두 tensor를 더하면 어떻게 될까요? 정답은 크기가 작은 B행렬이 크기가 큰 A행렬로 Broadcasting되어 [[2,4,6,8],[6,8,10,12]] 라는 결과를 얻을 수 있게 됩니다.\n",
    "\n",
    "4) Matrix multiplication\n",
    "\n",
    "위에서 설명한대로 Broadcasting을 수행할 때, 행렬의 곱의 상황은 어떨까요? 우선 tensor에는 행렬 곱을 위한 mm, matmul 두가지의 메서드가 존재합니다. (메서드에 대해 잘 모르시면 tensor라는 객체가 가지고 있는 함수 라고 생각하시면 됩니다.) mm 메서드의 경우 Broadcasting을 지원하지 않고 matmul 메서드의 경우 지원합니다. 그럼 위와 같은 상황 A, B를 곱한다면 어떻게 연산이 이루어 질까요? 한번 직접 코드를 통해 실습하고 결과를 분석해보세요 :)\n",
    "\n",
    "5) nn.Functional\n",
    "\n",
    "PyTorch에서는 다양한 function들을 한번에 모아서 사용 할 수 있도록 하는 모듈인 nn.Functional을 제공합니다! 이를 통해서 sigmoid, tanh, cross entropy 등 다양한 함수들을 직접 구현하지 않아도 사용 할 수 있습니다 :)   nn.~ 으로 제공되는 함수들은 어떤게 있을까요? \n",
    "\n",
    "6) AutoGrad\n",
    "\n",
    "PyTorch의 핵심 기능 중 하나죠! 바로 자동 미분입니다. pytorch는 어떤 tensor에 대한 연산 정보들을 기억했다가 자동으로 미분해주는 기능이 있습니다. 바로 Tensor 객체의 requires_grad 인자를 True로 만들어 주면 되는데요!  xx  텐서에 대해서  2x^22x\n",
    "​2\n",
    "​​  같은 연산을 거쳤다고 해보겠습니다. 해당 미분의 값은  4x4x 가 되겠죠? 그럼 해당 값인 \"4\"를 미분 값으로 잘 가지고 있다가 연산에 필요할 때 적용하게 됩니다. 더 다양한 예시들을 실습을 통해 적용 해 보시면서 자동 미분의 편리함을 체화해 보세요! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
